# Information Theory (COMP2610/COMP6261)

Information theory studies the fundamental limits of the representation and transmission of information. This course provides an introduction to information theory, studying fundamental concepts such as probability, information, and entropy and examining their applications in the areas of data compression, coding, communications, pattern recognition and probabilistic inference.

This is a public repository for an information theory course that has run at the ANU as a 2nd year undergraduate course in second semester since 2009. From 2014 it was double-badged as a Masters level course that contained extra material and assessment.

## Lecturers

I developed the material for this course with [Edwin Bonilla][] (2009-2012) and [Aditya Menon][] (2013-). Marcus Hutter contributed guest lectures on Kolmogorov complexity and algorithmic information theory (2009-).

[Edwin Bonilla]: http://ebonilla.github.io
[Aditya Menon]: http://people.cecs.anu.edu.au/user/5022

## Textbooks

The material in this course is largely based on David MacKay's excellent 
book [Information Theory, Inference, and Learning Algorithms][itila], which is
freely available to download.

As secondary texts, we use Cover & Thomas's [Elements of Information Theory][eit]
and Chris Bishop's [Pattern Recognition and Machine Learning][prml].

[itila]: http://www.inference.phy.cam.ac.uk/mackay/itila
[eit]: http://elementsofinformationtheory.com
[prml]: http://research.microsoft.com/en-us/um/people/cmbishop/PRML/

## Slides

- [Lecture 1](/bits/info/l01.pdf): Administration & Overview
- [Lecture 2](/bits/info/l02.pdf): Introduction & Motivation
- [Lecture 3](/bits/info/l03.pdf): Probability Theory & Bayes Rule
- [Lecture 4](/bits/info/l04.pdf): Bayesian Inference
- [Lecture 5](/bits/info/l05.pdf): Probability Distributions
- [Lecture 6](/bits/info/l06.pdf): Entropy
- [Lecture 7](/bits/info/l07.pdf): KL Divergence & Mutual Information
- [Lecture 8](/bits/info/l08.pdf): Fundamental inequalities
- [Lecture 9](/bits/info/l09.pdf): Probabilistic inequalities & Applications
- [Lecture 10](/bits/info/l10.pdf): Ensembles, Typicality & AEP
- [Lecture 11](/bits/info/l11.pdf): Introduction to compression; Uniform coding
- [Lecture 12](/bits/info/l12.pdf): The Source Coding Theorem
- [Lecture 13](/bits/info/l13.pdf): Symbol Codes for Lossless Compression
- [Lecture 14](/bits/info/l14.pdf): Source Coding Theorem for Symbol Codes
- [Lecture 15](/bits/info/l15.pdf): Interval and Arithmetic Coding
- [Lecture 16](/bits/info/l16.pdf): Arithmetic Coding (continued)
- [Lecture 17](/bits/info/l17.pdf): Lempel-Ziv Coding
- [Lecture 18](/bits/info/l18.pdf): Noisy Channels
- [Lecture 19](/bits/info/l19.pdf): The Noisy-Channel Coding Theorem
- [Lecture 20](/bits/info/l20.pdf): The Noisy-Channel Coding Theorem (continued)
- [Lecture 21](/bits/info/l21.pdf): Computing Channel Capacity; Review
- [Lecture 22](/bits/info/l22.pdf): Hamming Codes

